<!DOCTYPE html>
<!-- saved from url=(0017)https://xzhou.me/ -->
<html lang="en">

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><!--<base target="_blank">-->
    <base href="." target="_blank">
    <link rel="preconnect" href="https://fonts.gstatic.com/">
    <link rel="stylesheet" type="text/css" data-href="https://fonts.googleapis.com/css?family=Lato">
    <link rel="preconnect" href="https://fonts.gstatic.com/" crossorigin="">
    <meta name="viewport" content="width=device-width">
    <title>Hanxue Liang's Homepage</title>
    <meta name="description" content="I am a Ph.D. student in Computer Science and Technology at University of Cambridge">
    <meta name="next-head-count" content="4">
    <link rel="preload" href="./Homepage_files/fd4c9bf86266e80fae27.css" as="style">
    <link rel="stylesheet" href="./Homepage_files/fd4c9bf86266e80fae27.css" data-n-g="">
    <link rel="preload" href="./Homepage_files/a515be4c2f49419f3769.css" as="style">
    <link rel="stylesheet" href="./Homepage_files/a515be4c2f49419f3769.css" data-n-p=""><noscript
        data-n-css=""></noscript>
    <script defer="" nomodule="" src="./Homepage_files/polyfills-a54b4f32bdc1ef890ddd.js.下载"></script>
    <script src="./Homepage_files/webpack-715970c8028b8d8e1f64.js.下载" defer=""></script>
    <script src="./Homepage_files/framework-64eb7138163e04c228e4.js.下载" defer=""></script>
    <script src="./Homepage_files/main-c94e7f60255631414010.js.下载" defer=""></script>
    <script src="./Homepage_files/_app-3fa27215a3c41987e6d2.js.下载" defer=""></script>
    <script src="./Homepage_files/688-b942890a87f9a08b0e31.js.下载" defer=""></script>
    <script src="./Homepage_files/index-335648998a7bdac0c719.js.下载" defer=""></script>
    <script src="./Homepage_files/_buildManifest.js.下载" defer=""></script>
    <script src="./Homepage_files/_ssgManifest.js.下载" defer=""></script>
    <style data-href="https://fonts.googleapis.com/css?family=Lato">
        @font-face {
            font-family: 'Lato';
            font-style: normal;
            font-weight: 400;
            src: url(https://fonts.gstatic.com/s/lato/v24/S6uyw4BMUTPHjx4wWA.woff) format('woff')
        }

        @font-face {
            font-family: 'Lato';
            font-style: normal;
            font-weight: 400;
            src: url(https://fonts.gstatic.com/s/lato/v24/S6uyw4BMUTPHjxAwXiWtFCfQ7A.woff2) format('woff2');
            unicode-range: U+0100-02AF, U+0304, U+0308, U+0329, U+1E00-1E9F, U+1EF2-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF
        }

        @font-face {
            font-family: 'Lato';
            font-style: normal;
            font-weight: 400;
            src: url(https://fonts.gstatic.com/s/lato/v24/S6uyw4BMUTPHjx4wXiWtFCc.woff2) format('woff2');
            unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+0304, U+0308, U+0329, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD
        }
    </style>
</head>

<body>

    <div id="__next">
        <div class="styles_container__1_WuM">
            <section>
                <div class="personal_profile__BdQI4"><img class="personal_portrait__BAkO0"
                        src="Homepage_files/images/profile.jpg" alt="pottrait">
                    <div class="personal_profileInfo__1Y4pW">
                        <h2 class="personal_name__1_mHK">Hanxue (Charles) Liang</h2>
                        <!-- <h3 class="personal_chineseName__18aOD">樊志文</h3> -->
                        <h3 class="personal_worksFor__2L0De">University of Cambridge</h3>
                        <div class="personal_links__p_eYL"><span><a
                                    href="mailto:hl589@cam.ac.uk">Email</a></span><span><a
                                    href="https://scholar.google.com/citations?user=XcxDA14AAAAJ&hl=en">Google
                                    Scholar</a></span><span><a href="https://www.linkedin.com/in/hanxue-charles-liang-78b581177/">Linkedin</a></span></div>
                    </div>
                </div>
            </section>
            <section>
                <h2>About Me</h2>
                <div>
                    <p>I am a final-year PhD student in Computer Science at <a href="https://www.cam.ac.uk/">
                        University of Cambridge</a>, advised by <a href="https://www.cl.cam.ac.uk/~aco41/">Prof. Cengiz Öztireli</a> and <a href="https://www.cl.cam.ac.uk/~rkm38/">Prof. Rafal Mantiuk</a>.
                        Before starting my Ph.D., I obtained my Master's degree in Robotics, Systems and Control from <a href="https://ethz.ch/de.html">ETH Zurich</a>  under supervision of <a href="https://scholar.google.com/citations?user=TwMib_QAAAAJ&hl=en">Prof. Luc Van Gool</a>. <br>
                        I collaborate closely with <a href="https://huangjh-pub.github.io/">Dr. Jiahui Huang</a>, <a href="https://zgojcic.github.io/">Dr. Zan Gojcic</a> and <a href="https://www.cs.utoronto.ca/~fidler/">Prof. Sanja Fidler</a> on 3D/4D Reconstruction and Generation; with <a href="https://www.ece.utexas.edu/people/faculty/atlas-wang">Prof. Atlas Wang</a> and <a href="https://ych133.github.io/">Prof. Yu Cheng</a> on topic of on-device Mixture of Expert for Multi-Task/Modality Learning and Generalizable Novel View Synthesis; with <a href="https://fnzhan.com/">Dr. Fangneng Zhan </a> and <a href="https://people.mpi-inf.mpg.de/~theobalt/">Prof. Christian Theobalt</a> on Evolutive Rendering Optimization; with <a href="https://scholar.google.com/citations?user=TwMib_QAAAAJ&hl=en">Prof. Luc Van Gool</a> on Self-supervised Learning for Point Cloud Understanding.
                        I was the finalist of <a href="https://www.qualcomm.com/research/university-relations/innovation-fellowship/finalists">Qualcomm Innovation Fellowship 2024</a>.  <br>
                    </p>
                    My research focuses on computer vision, graphics and machine learning, with particular expertise in:
                        <ul>
                            <li>  3D/4D Reconstruction and Generation from Images/Monocular Video: <br> <a href="https://arxiv.org/pdf/2412.03526">Bullet-Timer</a>, <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Cong_Enhancing_NeRF_akin_to_Enhancing_LLMs_Generalizable_NeRF_Transformer_with_ICCV_2023_paper.pdf">GNT-MOVE</a>, <a href="https://arxiv.org/pdf/2405.17531">ERM</a>, <a href="https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.15036">Perceptual-NeRF</a>, <a href="https://arxiv.org/pdf/2303.10083">&alpha;Surf</a>, <a href="https://arxiv.org/pdf/2405.16645">Diffusion4D</a>, <a href="https://arxiv.org/pdf/2406.10324">L4GM</a>, <a href="https://ieeexplore.ieee.org/abstract/document/10459060">NeRF-NQA</a>
                            <li>  On-device Mixture of Experts for Multi-task/modality Learning Agent:  <br>  <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/b653f34d576d1790481e3797cb740214-Paper-Conference.pdf">M<sup>3</sup>ViT</a>, <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Cong_Enhancing_NeRF_akin_to_Enhancing_LLMs_Generalizable_NeRF_Transformer_with_ICCV_2023_paper.pdf">GNT-MOVE</a>, <a href="https://arxiv.org/pdf/2305.18691.pdf">Edge-MoE</a>
                            <li>  Self-supervised Learning for Point Cloud Understanding: <br> <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136630159.pdf">MLSP</a>, <a href="http://openaccess.thecvf.com/content/ICCV2021/papers/Liang_Exploring_Geometry-Aware_Contrast_and_Clustering_Harmonization_for_Self-Supervised_3D_Object_ICCV_2021_paper.pdf">GCC-3D</a>, <a href="https://arxiv.org/pdf/2106.11037">ONCE-Dataset</a>
                        </ul>
                </div>
            </section>
            <section>
                <h2>Recent News</h2>
                <ul>
                    <li>Our work <a href="https://arxiv.org/pdf/2303.10083">&alpha;Surf</a> is accepted by 3DV'25 as <span
                        class="publication_highlights__2ILmf"> oral </span>presentation. </li>
                    <li>Our work <a href="https://arxiv.org/pdf/2405.16645">Diffusion4D</a>, <a href="https://arxiv.org/pdf/2406.10324">L4GM</a>, <a href="https://arxiv.org/pdf/2410.20030">SCube</a> are accepted by NeurIPS'24. Congratulations to our collaborators. </li>
                    <li>I was one of the finalists of the <strong><span style="color: darkblue;">Qualcomm Innovation Fellowship</span></strong> 2024 <a href="https://www.qualcomm.com/research/university-relations/innovation-fellowship/finalists">(QIF 2024)</a>. Innovation title: "Real-time On-device 3D Modeling via Neural Implicit Scene Representations". </li>
                    <li>Our work <a href="https://ieeexplore.ieee.org/abstract/document/10459060">NeRF-NQA</a> is accepted by Transactions on Visualization and Computer Graphics (TVCG) 24. </li>
                    <li>Our work <a href="https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.15036">Perceptual-NeRF</a> is accepted by EUROGRAPHICS'24. </li>
                    <li>Our work <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Cong_Enhancing_NeRF_akin_to_Enhancing_LLMs_Generalizable_NeRF_Transformer_with_ICCV_2023_paper.pdf">GNT-MOVE</a> is accepted by ICCV'23. </li>
                    <li>We won 3rd place in the <strong><span style="color: darkblue;">University Demo Best Demonstration</span></strong> at the 59th Design Automation Conference <a href="https://www.dac.com/">(DAC 2022)</a>. We demo for a multi-task vision transformer on FPGA. </li>
                    <li>Our work <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/b653f34d576d1790481e3797cb740214-Paper-Conference.pdf">M<sup>3</sup>ViT</a> is accepted by NeurIPS'22. </li>
                    <li>Our work <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136630159.pdf">MLSP</a> is accepted by ECCV'22. </li>
                    <!--<li>Our ECCV'22 (<a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Fan_CADTransformer_Panoptic_Symbol_Spotting_Transformer_for_CAD_Drawings_CVPR_2022_paper.pdf">CADTransformer</a>) is selected as <strong>oral</strong> presentation. </li>-->
                    <!--<li>Our paper for CVPR'20 (<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Gu_Cascade_Cost_Volume_for_High-Resolution_Multi-View_Stereo_and_Stereo_Matching_CVPR_2020_paper.pdf">Cascade Cost Volume</a>) is selected as <strong>oral</strong> presentation. </li>-->
                </ul> 
                <!-- <div class="styles_showMore__JvZFs"><button>Show More</button></div> -->
            </section>
            <section>
                <h2>Research Overview</h2>
                <div style="text-align: center;">
                    <img src="Homepage_files/images/researchoverview.png" alt="Research Overview Image" style="max-width: 60%; height: auto;">
                </div>
            </section>

            <!--
            <section>
                <h2>Researches and Projects<br class="publication-list_mobileBreak__24vsO"></h2>


                <div class="publication_publication__1Icb_">
                    <div class="publication_image__1EUuC"><img src="./Homepage_files/rs_few_shot_3d.svg"
                        alt="loading...">
                </div>
                    <div class="publication_info__kLRGP">
                        <div class="publication_title__3m6SE">Few-Shot 3D Learning with Geometric and Generative Priors</div>
                        <div class="publication_authors__qkFXc">Creating photorealistic 3D environments often lacks dense scene
                            capture with annotated poses. My research integrates geometric principles with generative priors from
                            large datasets, enabling 3D/4D asset creation from multi-modal inputs by leveraging statistical patterns
                            and combining deterministic geometry with generative models for efficient learning.</div>
                        <div class="publication_links__aEpO_">
                            <ul>
                                <li> <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Gu_Cascade_Cost_Volume_for_High-Resolution_Multi-View_Stereo_and_Stereo_Matching_CVPR_2020_paper.pdf">CasMVSNet </a>(CVPR'20, Oral), <a href="https://ieeexplore-ieee-org.ezproxy.lib.utexas.edu/abstract/document/10550594/">Cas6D</a> (3DV'24).</li>
                                <li> <a href="https://vita-group.github.io/SinNeRF/">SinNeRF </a> (ECCV'22), <a href="https://vita-group.github.io/NeuralLift-360/">NeuralLift-360 </a> (CVPR'23, Highlight). </li>
                                <li> <a href="https://zehaozhu.github.io/FSGS/">FSGS </a> (ECCV'24), <a href="https://dreamscene360.github.io/">DreamScene360 </a> (ECCV'24). </li>
                                <li> <a href="https://4k4dgen.github.io/">4K4DGen </a> (Preprint) </li>
                            </ul>

                                </div>
                    </div>
                </div>


                <div class="publication_publication__1Icb_">
                    <div class="publication_image__1EUuC"><img src="./Homepage_files/rs_is+lg.svg"
                        alt="loading...">
                </div>
                    <div class="publication_info__kLRGP">
                        <div class="publication_title__3m6SE">Ultra-Efficient 3D Reconstruction and Rendering</div>
                        <div class="publication_authors__qkFXc">Foundation models for 3D data, such as web videos and robotic RGB-D,
                            require efficient end-to-end pipelines. Current modular approaches for pose estimation and dense reconstruction
                            are slow and prone to errors. My research introduces an end-to-end framework that optimizes 3D structures and
                            camera parameters under self-supervision, reducing training time from hours to seconds. It also improves rendering
                            speed by 60% and compresses 3D models by over 15 times, enabling scalable, high-performance 3D systems.
                        </div>
                        <div class="publication_links__aEpO_">
                            <ul>
                                <li> <a href="https://lightgaussian.github.io/">LightGaussian </a> (NeurIPS'24, Spotlight) </li>
                                <li> <a href="https://instantsplat.github.io/">InstantSplat </a>(Preprint), VideoLifter (submitted).</li>
                            </ul>
                        </div>
                    </div>
                </div>

                <div class="publication_publication__1Icb_">
                    <div class="publication_image__1EUuC"><img src="./Homepage_files/rs_lsm.svg"
                        alt="loading...">
                </div>
                    <div class="publication_info__kLRGP">
                        <div class="publication_title__3m6SE">Semantic 3D Foundation Model: Reconstruction, Understanding and Beyond</div>
                        <div class="publication_authors__qkFXc">Humans perceive, reason, and interact with their environment through an innate understanding of structure and properties. To equip intelligent machines with similar abilities, it is vital to enable direct geometry perception from visual inputs, bypassing offline pose preprocessing for scalable and reliable planning.

                            Next-generation learning algorithms shall inherently perceive geometric structures, pre-train 3D models on Internet-scale video data, and integrate with visual-language models for reasoning and planning. Advancing architectures to process and interpret high-resolution temporal visual streams is key to enabling robust 3D understanding and interaction.
                        </div>
                        <div class="publication_links__aEpO_">
                            <ul>
                                <li> <a href="https://feature-3dgs.github.io/">Feature 3DGS </a> (CVPR'24, Highlight), <a href="https://openreview.net/forum?id=kfOtMqYJlUU">NeRF-SOS </a> (ICLR'23). </li>
                                <li> <a href="https://largespatialmodel.github.io/">Large Spatial Model </a>(NeurIPS'24).</li>
                                <li> Geometric Language Model (ongoing).</li>
                            </ul>
                        </div>
                    </div>
                </div>


                <div>
                    <div class="publication_publication__1Icb_">
                        <div class="publication_image__full_size"><video src="./Homepage_files/videos/homepage_applications.mp4"
                            title="Video demos for application is loading.." playsinline="" autoplay="" loop="" preload=""
                            muted=""></video>
                    </div>
                </div>
                <p>My research has been demonstrated on platforms such as Quest 3, implemented within IARPA projects, and integrated into multiple commercial products.</p>




            </section>
             -->

            <section>
                <h2>Selected Publications<br class="publication-list_mobileBreak__24vsO"><span
                        class="publication-list_filters__3ikvu"><span>Full publication list at </span><span><a
                                href="https://scholar.google.com/citations?user=XcxDA14AAAAJ&hl=en">Google
                                Scholar</a></span></span></h2>
                <div class="publication-list_smallText__pUJXB">* denotes equal contribution, &dagger; denotes project lead.</div>
                <p>3D/4D Reconstruction and Generation from Images/Monocular Video</p>
                <div>

                    <div class="publication_publication__1Icb_">
                        <div class="publication_image__1EUuC"><video src="./Homepage_files/videos/bullettimer.mp4"
                            title="Bullettimer video loading.." playsinline="" autoplay="" loop="" preload=""
                            muted=""></video></div>
                        <div class="publication_info__kLRGP">
                            <div class="publication_title__3m6SE">Feed-Forward Bullet-Time Reconstruction of Dynamic Scenes from Monocular Videos</div>
                            <div class="publication_authors__qkFXc"><span><span>Hanxue</span>
                                <span>Liang*<sup>&dagger;</sup></span></span><span><span>Jiawei </span>
                                <span>Ren*</span></span><span><span>Ashkan </span>
                                <span>Mirzaei*</span></span><span><span>Antonio </span>
                                <span>Torralba</span></span><span><span>Ziwei </span>
                                <span>Liu</span></span><span><span>Igor </span>
                                <span>Gilitschenski</span></span><span><span>Sanja </span>
                                <span>Fidler</span></span><span><span>Cengiz </span>
                                <span>Oztireli</span></span><span><span>Huan </span>
                                <span>Ling</span></span><span><span>Zan </span>
                                <span>Gojcic</span></span><span><span>Jiahui </span>
                                Huang</span></div>
                            <div class="publication_venue__1Dv6R"><span>Arxiv, 2024</span><span></span>&nbsp;</div>
                            <div class="publication_links__aEpO_"><span class="publication_link__fXY43"><a
                                        href="https://arxiv.org/pdf/2412.03526">Paper</a></span><span
                                    class="publication_link__fXY43"><a
                                        href="https://research.nvidia.com/labs/toronto-ai/bullet-timer/">Project</a></span><span></div>
                        </div>
                    </div>

                    <div class="publication_publication__1Icb_">
                        <div class="publication_image__1EUuC"><video src="./Homepage_files/videos/ERM.mp4"
                            title="ERM video loading.." playsinline="" autoplay="" loop="" preload=""
                            muted=""></video></div>
                        <div class="publication_info__kLRGP">
                            <div class="publication_title__3m6SE">Evolutive Rendering Models</div>
                            <div class="publication_authors__qkFXc"><span><span>Hanxue</span>
                                <span>Liang*</span></span><span><span>Fangneng </span>
                                <span>Zhan*</span></span><span><span>Yifan </span>
                                <span>Wang</span></span><span><span>Michael </span>
                                <span>Niemeyer</span></span><span><span>Michael </span>
                                <span>Oechsle</span></span><span><span>Adam </span>
                                <span>Kortylewski</span></span><span><span>Cengiz </span>
                                <span>Oztireli</span></span><span><span>Gordon </span>
                                <span>Wetzstein</span></span><span><span>Christian </span>
                                <span>Theobalt</span></div>
                            <div class="publication_venue__1Dv6R"><span>Arxiv, 2024</span><span></span>&nbsp;</div>
                            <div class="publication_links__aEpO_"><span class="publication_link__fXY43"><a
                                        href="https://arxiv.org/pdf/2405.17531">Paper</a></span><span
                                    class="publication_link__fXY43"><a
                                        href="https://fnzhan.com/Evolutive-Rendering-Models/">Project</a></span><span></div>
                        </div>
                    </div>

                    <div class="publication_publication__1Icb_">
                        <div class="publication_image__1EUuC"><video src="./Homepage_files/videos/perceptualnerf.mp4.MOV"
                            title="PerceptualNeRF video loading.." playsinline="" autoplay="" loop="" preload=""
                            muted=""></video></div>
                        <div class="publication_info__kLRGP">
                            <div class="publication_title__3m6SE">Perceptual Quality Assessment of NeRF and Neural View Synthesis Methods for Front-Facing Views</div>
                            <div class="publication_authors__qkFXc"><span><span>Hanxue</span>
                                <span>Liang</span></span><span><span>Tianhao </span>
                                <span>Wu</span></span><span><span>Param </span>
                                <span>Hanji</span></span><span><span>Francesco </span>
                                <span>Banterle</span></span><span><span>Hongyun </span>
                                <span>Gao</span></span><span><span>Rafał </span>
                                <span>Mantiuk</span></span><span><span>Cengiz </span>
                                <span>Oztireli</span></div>
                            <div class="publication_venue__1Dv6R"><span>EUROGRAPHICS 2024</span><span></span>&nbsp;</div>
                            <div class="publication_links__aEpO_"><span class="publication_link__fXY43"><a
                                        href="https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.15036">Paper</a></span><span
                                    class="publication_link__fXY43"><a
                                        href="https://www.cl.cam.ac.uk/research/rainbow/projects/perceptualnerf/">Project</a></span><span></div>
                        </div>
                    </div>

                    <div class="publication_publication__1Icb_">
                        <div class="publication_image__1EUuC"><img src="./Homepage_files/images/gntmove.png"
                            alt="GNT-MOVE framework loading...">
                    </div>
                        <div class="publication_info__kLRGP">
                            <div class="publication_title__3m6SE">Enhancing NeRF akin to Enhancing LLMs: Generalizable NeRF Transformer with Mixture-of-View-Experts</div>
                            <div class="publication_authors__qkFXc"><span><span>Hanxue</span>
                                    <span>Liang*<sup>&dagger;</sup></span></span><span><span>Wenyan</span>
                                    <span>Cong*</span></span><span><span>Peihao<span>
                                    </span>Wang</span></span><span><span></span>Zhiwen<span>
                                    </span>Fan</span></span><span><span></span>Tianlong<span>
                                    </span>Chen</span></span><span><span></span>Mukund<span>
                                    </span>Varma</span></span><span><span></span>Yi<span>
                                    </span>Wang</span></span><span><span></span>Zhangyang<span>
                                    </span>Wang</span></div>
                            <div class="publication_venue__1Dv6R"><span>ICCV</span><span>2023</span>&nbsp; </div>
                            <div class="publication_links__aEpO_"><span class="publication_link__fXY43"><a
                                        href="https://openaccess.thecvf.com/content/ICCV2023/papers/Cong_Enhancing_NeRF_akin_to_Enhancing_LLMs_Generalizable_NeRF_Transformer_with_ICCV_2023_paper.pdf">Paper</a></span><span
                                    class="publication_link__fXY43"><a
                                        href="https://github.com/VITA-Group/GNT-MOVE">Code</a></span></div>
                        </div>
                    </div>

                    <div class="publication_publication__1Icb_">
                        <div class="publication_image__1EUuC"><video src="./Homepage_files/videos/alphasurf.mp4"
                            title="alphasurf video loading.." playsinline="" autoplay="" loop="" preload=""
                            muted=""></video></div>
                        <div class="publication_info__kLRGP">
                            <div class="publication_title__3m6SE">&alpha;Surf: Implicit Surface Reconstruction for Semi-Transparent and Thin Objects with Decoupled Geometry and Opacity</div>
                            <div class="publication_authors__qkFXc"><span><span>Tianhao</span>
                                <span>Wu</span></span><span><span>Hanxue </span>
                                <span>Liang</span></span><span><span>Fangcheng </span>
                                <span>Zhong</span></span><span><span>Gernot </span>
                                <span>Riegler</span></span><span><span>Shimon </span>
                                <span>Vainer</span></span><span><span>Jiankang </span>
                                <span>Deng</span></span><span><span>Cengiz </span>
                                <span>Oztireli</span></div>
                            <div class="publication_venue__1Dv6R"><span>3DV 2025</span><span></span>&nbsp;
                                <span
                                class="publication_highlights__2ILmf">(Oral Presentation)</span></div>
                            <div class="publication_links__aEpO_"><span class="publication_link__fXY43"><a
                                        href="https://arxiv.org/pdf/2303.10083">Paper</a></span><span
                                    class="publication_links__aEpO_"><span class="publication_link__fXY43"><a
                                            href="https://alphasurf.netlify.app/">Project</a></span><span
                                    class="publication_link__fXY43"><a
                                        href="https://github.com/ChikaYan/alphasurf">Code</a></span><span></div>
                        </div>
                    </div>

                    <div class="publication_publication__1Icb_">
                        <div class="publication_image__1EUuC"><video src="./Homepage_files/videos/diffusion4d.mp4"
                            title="PerceptualNeRF video loading.." playsinline="" autoplay="" loop="" preload=""
                            muted=""></video></div>
                        <div class="publication_info__kLRGP">
                            <div class="publication_title__3m6SE">Diffusion4D: Fast Spatial-temporal Consistent 4D Generation via Video Diffusion Models</div>
                            <div class="publication_authors__qkFXc"><span><span>Hanwen</span>
                                <span>Liang*</span></span><span><span>Yuyang </span>
                                <span>Yin*</span></span><span><span>Dejia </span>
                                <span>Xu</span></span><span><span>Hanxue </span>
                                <span>Liang</span></span><span><span>Zhangyang </span>
                                <span>Wang</span></span><span><span>Konstantinos </span>
                                <span>N. Plataniotis</span></span><span><span>Yao </span>
                                <span>Zhao</span></span><span><span>Yunchao </span>
                                <span>Wei</span></div>
                            <div class="publication_venue__1Dv6R"><span>NeurIPS 2024</span><span></span>&nbsp;</div>
                            <div class="publication_links__aEpO_"><span class="publication_link__fXY43"><a
                                        href="https://arxiv.org/pdf/2405.16645">Paper</a></span><span
                                    class="publication_link__fXY43"><a
                                        href="https://vita-group.github.io/Diffusion4D/">Project</a></span><span
                                    class="publication_link__fXY43"><a
                                            href="https://github.com/VITA-Group/Diffusion4D">Code</a></span><span></div>
                        </div>
                    </div>

                    <div class="publication_publication__1Icb_">
                        <div class="publication_image__1EUuC"><video src="./Homepage_files/videos/L4GM.mp4"
                            title="L4GM video loading.." playsinline="" autoplay="" loop="" preload=""
                            muted=""></video></div>
                        <div class="publication_info__kLRGP">
                            <div class="publication_title__3m6SE">L4GM: Large 4D Gaussian Reconstruction Model</div>
                            <div class="publication_authors__qkFXc"><span><span>Jiawei</span>
                                <span>Ren</span></span><span><span>Kevin </span>
                                <span>Xie</span></span><span><span>Ashkan </span>
                                <span>Mirzaei</span></span><span><span>Hanxue </span>
                                <span>Liang</span></span><span><span>Xiaohui </span>
                                <span>Zeng</span></span><span><span>Karsten </span>
                                <span>Kreis</span></span><span><span>Ziwei </span>
                                <span>Liu</span></span><span><span>Antonio </span>
                                <span>Torralba</span></span><span><span>Sanja </span>
                                <span>Fidler</span></span><span><span>Seung Wook </span>
                                <span>Kim</span></span><span><span>Huan </span>
                                <span>Ling</span></div>
                            <div class="publication_venue__1Dv6R"><span>NeurIPS 2024</span><span></span>&nbsp;</div>
                            <div class="publication_links__aEpO_"><span class="publication_link__fXY43"><a
                                        href="https://openreview.net/pdf?id=PSPtj26Lbp">Paper</a></span><span
                                    class="publication_links__aEpO_"><span class="publication_link__fXY43"><a
                                            href="https://research.nvidia.com/labs/toronto-ai/l4gm/">Project</a></span><span
                                    class="publication_link__fXY43"><a
                                        href="https://github.com/nv-tlabs/L4GM-official">Code</a></span><span></div>
                        </div>
                    </div>

                    <div class="publication_publication__1Icb_">
                        <div class="publication_image__1EUuC"><video src="./Homepage_files/videos/scube.mp4"
                            title="PerceptualNeRF video loading.." playsinline="" autoplay="" loop="" preload=""
                            muted=""></video></div>
                        <div class="publication_info__kLRGP">
                            <div class="publication_title__3m6SE">SCube: Instant Large-Scale Scene Reconstruction using VoxSplats</div>
                            <div class="publication_authors__qkFXc"><span><span>Xuanchi</span>
                                <span>Ren*</span></span><span><span>Yifan </span>
                                <span>Lu*</span></span><span><span>Hanxue </span>
                                <span>Liang</span></span><span><span>Zhangjie </span>
                                <span>Wu</span></span><span><span>Huan </span>
                                <span>Ling</span></span><span><span>Mike </span>
                                <span>Chen</span></span><span><span>Sanja </span>
                                <span>Fidler</span></span><span><span>Francis </span>
                                <span>Williams</span></span><span><span>Jiahui </span>
                                <span>Huang</span></div>
                            <div class="publication_venue__1Dv6R"><span>NeurIPS 2024</span><span></span>&nbsp;</div>
                            <div class="publication_links__aEpO_"><span class="publication_link__fXY43"><a
                                        href="https://arxiv.org/pdf/2410.20030">Paper</a></span><span
                                    class="publication_link__fXY43"><a
                                        href="https://research.nvidia.com/labs/toronto-ai/scube/">Project</a></span><span
                                    class="publication_link__fXY43"><a
                                            href="https://github.com/nv-tlabs/SCube">Code</a></span><span></div>
                        </div>
                    </div>

                    
                    <div class="publication_publication__1Icb_">
                        <div class="publication_image__1EUuC"><img src="./Homepage_files/images/nerfnqa.png"
                            alt="GNT-MOVE framework loading...">
                    </div>
                        <div class="publication_info__kLRGP">
                            <div class="publication_title__3m6SE">NeRF-NQA: No-Reference Quality Assessment for Scenes Generated by NeRF and Neural View Synthesis Methods</div>
                            <div class="publication_authors__qkFXc"><span><span>Qiang</span>
                                    <span>Qu</span></span><span><span>Hanxue</span>
                                    <span>Liang</span></span><span><span>Xiaoming<span>
                                    </span>Chen</span></span><span><span></span>Yuk Ying<span>
                                    </span>Chung</span></span><span><span></span>Yiran<span>
                                    </span>Shen</span></div>
                            <div class="publication_venue__1Dv6R"><span>TVCG</span><span>2024</span>&nbsp; </div>
                            <div class="publication_links__aEpO_"><span class="publication_link__fXY43"><a
                                        href="https://ieeexplore.ieee.org/abstract/document/10459060">Paper</a></span><span
                                    class="publication_link__fXY43"><a
                                        href="https://github.com/VincentQQu/NeRF-SNQA">Code</a></span></div>
                        </div>
                    </div>
                    
                    <p>On-device Mixture of Experts for Multi-task/modality Learning Agent</p>
                    <div class="publication_publication__1Icb_">
                        <div class="publication_image__1EUuC"><img src="./Homepage_files/images/m3vit.png"
                                alt="M^3ViT Video thumbnail loading...">
                        </div>
                        <div class="publication_info__kLRGP">
                            <div class="publication_title__3m6SE">M<sup>3</sup>ViT: Mixture-of-Experts Vision Transformer for Efficient Multi-task Learning with Model-Accelerator Co-design</div>
                            <div class="publication_authors__qkFXc"><span><span>Hanxue </span>
                                    <span>Liang*<sup>&dagger;</sup></span></span><span><span>Zhiwen </span>
                                    <span>Fan*</span></span><span><span>Rishov </span>
                                    <span>Sarkar</span></span><span><span>Ziyu </span>
                                    <span>Jiang</span></span><span><span>Tianlong </span>
                                    <span>Chen</span></span><span><span>Kai </span>
                                    <span>Zou</span></span><span><span>Yu </span>
                                    <span>Cheng</span></span><span><span>Cong  </span>
                                    <span>Hao</span></span><span><span>Zhangyang   </span>
                                    <span>Wang</span>
                                </div>
                            <div class="publication_venue__1Dv6R"><span>NeurIPS</span><span>2022</span>&nbsp;
                                <span
                                class="publication_highlights__2ILmf">(QIF 2022 Award & DAC 3rd best demo)</span>
                            </div>
                            <div class="publication_links__aEpO_"><span class="publication_link__fXY43"><a
                                        href="https://arxiv.org/pdf/2210.14793">Paper</a></span><span
                                    class="publication_link__fXY43"><a
                                        href="https://github.com/VITA-Group/M3ViT">Code</a></span></div>
                        </div>
                    </div>

                    <div class="publication_publication__1Icb_">
                        <div class="publication_image__1EUuC"><img src="./Homepage_files/images/edgemoe.png"
                                alt="M^3ViT Video thumbnail loading...">
                        </div>
                        <div class="publication_info__kLRGP">
                            <div class="publication_title__3m6SE">Edge-MoE: Memory-Efficient Multi-Task Vision Transformer Architecture with Task-level Sparsity via Mixture-of-Experts</div>
                            <div class="publication_authors__qkFXc"><span><span>Rishov </span>
                                    <span>Sarkar</span></span><span><span>Hanxue </span>
                                    <span>Liang</span></span><span><span>Zhiwen </span>
                                    <span>Fan</span></span><span><span>Zhangyang  </span>
                                    <span>Wang</span></span><span><span>Cong   </span>
                                    <span>Hao</span>
                                </div>
                            <div class="publication_venue__1Dv6R"><span>ICCAD</span><span>2023</span>&nbsp;
                            </div>
                            <div class="publication_links__aEpO_"><span class="publication_link__fXY43"><a
                                        href="https://arxiv.org/pdf/2305.18691">Paper</a></span><span
                                    class="publication_link__fXY43"><a
                                        href="https://github.com/sharc-lab/Edge-MoE">Code</a></span></div>
                        </div>
                    </div>
                    <p>Self-supervised Learning for Point Cloud Understanding</p>
                    <div class="publication_publication__1Icb_">
                        <div class="publication_image__1EUuC"><img src="./Homepage_files/images/MLSP.png"
                                alt="MLSP image loading...">
                        </div>
                        <div class="publication_info__kLRGP">
                            <div class="publication_title__3m6SE">Point Cloud Domain Adaptation via Masked Local 3D Structure Prediction</div>
                            <div class="publication_authors__qkFXc"><span><span>Hanxue </span>
                                    <span>Liang</span></span><span><span>Hehe </span>
                                    <span>Fan</span></span><span><span>Zhiwen </span>
                                    <span>Fan</span></span><span><span>Yi </span>
                                    <span>Wang</span></span><span><span>Tianlong </span>
                                    <span>Chen</span></span><span><span>Yu </span>
                                    <span>Cheng</span></span><span><span>Zhangyang </span>
                                    <span>Wang</span>
                                </div>
                            <div class="publication_venue__1Dv6R"><span>ECCV</span><span>2022</span>&nbsp;
                            </div>
                            <div class="publication_links__aEpO_"><span class="publication_link__fXY43"><a
                                        href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136630159.pdf">Paper</a></span><span
                                    class="publication_link__fXY43"><a
                                        href="https://github.com/VITA-Group/MLSP">Code</a></span></div>
                        </div>
                    </div>

                    <div class="publication_publication__1Icb_">
                        <div class="publication_image__1EUuC"><img src="./Homepage_files/images/GCC3D.png"
                                alt="M^3ViT Video thumbnail loading...">
                        </div>
                        <div class="publication_info__kLRGP">
                            <div class="publication_title__3m6SE">Exploring Geometry-aware Contrast and Clustering Harmonization for Self-supervised 3D Object Detection</div>
                            <div class="publication_authors__qkFXc"><span><span>Hanxue </span>
                                    <span>Liang*</span></span><span><span>Chenhan </span>
                                    <span>Jiang*</span></span><span><span>Dapeng </span>
                                    <span>Feng</span></span><span><span>Xin </span>
                                    <span>Chen</span></span><span><span>Hang </span>
                                    <span>Xu</span></span><span><span>Xiaodan </span>
                                    <span>Liang</span></span><span><span>Wei </span>
                                    <span>Zhang</span></span><span><span>Zhenguo  </span>
                                    <span>Li</span></span><span><span>Luc Van   </span>
                                    <span>Gool</span>
                                </div>
                            <div class="publication_venue__1Dv6R"><span>ICCV</span><span>2021</span>&nbsp;
                                </div>
                                <div class="publication_links__aEpO_"><span class="publication_link__fXY43"><a
                                            href="https://openaccess.thecvf.com/content/ICCV2021/papers/Liang_Exploring_Geometry-Aware_Contrast_and_Clustering_Harmonization_for_Self-Supervised_3D_Object_ICCV_2021_paper.pdf">Paper</a>
                                        </div>
                            </div>
                    </div>

                    <div class="publication_publication__1Icb_">
                        <div class="publication_image__1EUuC"><img src="./Homepage_files/images/once.png"
                                alt="M^3ViT Video thumbnail loading...">
                        </div>
                        <div class="publication_info__kLRGP">
                            <div class="publication_title__3m6SE">One Million Scenes for Autonomous Driving:ONCE Dataset
                                </div>
                            <div class="publication_authors__qkFXc"><span><span>Jiageng </span>
                                    <span>Mao</span></span><span><span>Minzhe </span>
                                    <span>Niu</span></span><span><span>Chenhan </span>
                                    <span>Jiang</span></span><span><span>Hanxue </span>
                                    <span>Liang</span></span><span><span>Jingheng </span>
                                    <span>Chen</span></span><span><span>Xiaodan </span>
                                    <span>Liang</span></span><span><span>Yamin </span>
                                    <span>Li</span></span><span><span>Chaoqiang  </span>
                                    <span>Ye</span></span><span><span>Wei </span>
                                    <span>Zhang</span></span><span><span>Zhenguo </span>
                                    <span>Li</span></span><span><span>Jie </span>
                                    <span>Yu</span></span><span><span>Hang </span>
                                    <span>Xu</span></span><span><span>Chunjing </span>
                                    <span>Xu</span>
                                </div>
                                <div class="publication_venue__1Dv6R"><span>NeurIPS Datatrack</span><span>2021</span>&nbsp;
                                </div>
                                <div class="publication_links__aEpO_"><span class="publication_link__fXY43"><a
                                            href="https://arxiv.org/pdf/2106.11037">Paper</a></span><span
                                        class="publication_link__fXY43"><a
                                            href="https://once-for-auto-driving.github.io/index.html">Project</a></span></div>
                            </div>
                    </div>
                    
                </div>
            </section>
            <!--
            <section>
                <h2>Invited Talks</h2>
                <ul>
                    <li>
                        <div style="display:inline">
                            Scalable 3D/4D Assets Creation @ <strong>Duke</strong>. November 2024</span>.
                    </li>

                    <li>
                        <div style="display:inline">
                            E cient 3D Learning for Autonomous System @ <strong>UNC, Guest Lecture</strong>. November 2024</span>.
                    </li>

                    <li>
                        <div style="display:inline">
                            Empowering Machines to Understand 3D @ <strong>Stanford, ASU, JHU, Yale</strong>. October 2024</span>.
                    </li>

                    <li>
                        <div style="display:inline">
                            3D Computer Vision @ <strong>TAMU, Guest Lecture</strong>. October 2024</span>.
                    </li>

                    <li>
                        <div style="display:inline">
                            From Efficient 3D Learning to 3D Foundation Models @ <strong>UCLA and CalTech</strong>. October 2024</span>.
                    </li>

                    <li>
                        <div style="display:inline">
                            Towards Universal, Real-Time 3D Construction and Interaction @ <strong>TAMU AI Lunch</strong>. October 2024</span>.
                    </li>

                    <li>
                        <div style="display:inline">
                            Spatial Intelligence via Reconstruction, Distillation, and Generation @ <strong>Shanghai AI Lab</strong>. July 2024</span>.
                    </li>

                    <li>
                        <div style="display:inline">
                            Streamlined 3D/4D: From Hours to Seconds to Millisecond @ <strong>Google Research, <a
                                href="https://valser.org/article-761-1.html">VALSE Webinar </a></strong>. May 2024</span>.
                    </li>

                    <li>
                        <div style="display:inline">
                            Streamlined 3D/4D: From Hours to Seconds to Millisecond @ <strong>Google Research</strong>. May 2024</span>.
                    </li>

                    <li>
                        <div style="display:inline">
                            Real-Time Few-shot View Synthesis w/ Gaussian Splatting @ <strong>IARPA WRIVA Workshop</strong>. April 2024</span>.
                    </li>

                    <li>
                        <div style="display:inline">
                            Data-efficient and Rendering-efficient Neural Rendering @ <strong>IFML Workshop on Gen AI</strong>. November 2023</span>.
                    </li>

                    <li>
                        <div style="display:inline">
                            Unified Implicit Neural Stylization @ <strong>Xiamen University; Kungfu.ai.</strong> July 2022</span>.
                    </li>


                </ul>
            </section>
            -->
            <section>
                <h2>Experience</h2>
                <ul>
                    <li>
                        <div style="display:inline">
                            Qualcomm<!-- -->: </div><span
                            class="styles_collaborator__VflHz">Research Intern (year of 2024)</span>.
                    </li>
                    <li>
                        <div style="display:inline">
                            Nvidia, Toronto AI Lab<!-- -->: </div><span
                            class="styles_collaborator__VflHz">Research Scientist Intern (year of 2024)</span>.
                    </li>

                    <li>
                        <div style="display:inline">
                            Max Planck Institute for Informatics; Google Research<!-- -->: </div><span
                            class="styles_collaborator__VflHz">Research Intern (2023-2024)</span>.
                    </li>

                    <li>
                        <div style="display:inline">
                            Google Research<!-- -->: </div><span
                            class="styles_collaborator__VflHz">Student Researcher (year of 2023)</span>.
                    </li>

                    <li>
                        <div style="display:inline">
                            Microsoft Research<!-- -->: </div><span
                            class="styles_collaborator__VflHz">Research Intern (2021-2022)</span>.
                    </li>


                    <li>
                        <div style="display:inline">
                            Microsoft Research<!-- -->: </div><span
                            class="styles_collaborator__VflHz">Research Intern (year of 2019)</span>.
                    </li>
                </ul>
            </section>
            <section>
                <h2>Services</h2>
                <ul>
                    <!--
                    <li>
                        <div style="display:inline">
                            Journal Reviewers:&nbsp;</div><span
                            class="styles_collaborator__VflHz">TPAMI, TIP, IJCV, Neurocomputing</span>.
                    </li>-->
                    <li>
                        <div style="display:inline">
                            Conference  Reviewers:&nbsp;</div><span
                            class="styles_collaborator__VflHz">NeurIPS 23/24, ICLR 24/25, CVPR 24/25, ECCV 24 </span>.
                    </li>
                </ul>
            </section>
            <footer class="styles_footer__3qp3V">
                <p>Last updated on <time datetime="Sep 10 2023">Dec 06, 2024</time></p>
                <p> The website template was originally borrowed from <a href="https://www.xzhou.me/">[1] </a>and <a href="https://zhiwenfan.github.io/">[2]</a>, thanks!.</p>
            </footer>
        </div>
    </div>

</body>

</html>